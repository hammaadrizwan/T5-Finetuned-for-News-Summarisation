{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Summarization Project\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets torch rouge-score accelerate pandas numpy\n!pip install transformers[torch] -U\n!pip install accelerate -U\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:42:39.655901Z","iopub.execute_input":"2025-07-08T04:42:39.656298Z","iopub.status.idle":"2025-07-08T04:43:02.058361Z","shell.execute_reply.started":"2025-07-08T04:42:39.656267Z","shell.execute_reply":"2025-07-08T04:43:02.057078Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=1de150359787844b17db36c3d53c0494090f5636118f3341e80b112dd118fb5e\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\nRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\nCollecting transformers[torch]\n  Downloading transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.16.1)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers[torch])\n  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers[torch])\n  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.67.1)\nRequirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.5.1+cu121)\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.30.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.12.2)\nCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers[torch])\n  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers[torch]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers[torch]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers[torch]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers[torch]) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers[torch]) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers[torch]) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1->transformers[torch]) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers[torch]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers[torch]) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers[torch]) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers[torch]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers[torch]) (2024.2.0)\nDownloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: hf-xet, huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.27.0\n    Uninstalling huggingface-hub-0.27.0:\n      Successfully uninstalled huggingface-hub-0.27.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed hf-xet-1.1.5 huggingface-hub-0.33.2 tokenizers-0.21.2 transformers-4.53.1\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.33.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.12.14)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\nSuccessfully installed accelerate-1.8.1\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom rouge_score import rouge_scorer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:43:08.098733Z","iopub.execute_input":"2025-07-08T04:43:08.099054Z","iopub.status.idle":"2025-07-08T04:43:08.103425Z","shell.execute_reply.started":"2025-07-08T04:43:08.099029Z","shell.execute_reply":"2025-07-08T04:43:08.102587Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## View Dataset Details","metadata":{}},{"cell_type":"code","source":"\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:2000]\")  \n\nprint(\"Dataset Info:\")\nprint(dataset)\n\narticles = dataset['article']\nhighlights = dataset['highlights']\nprint(\"\\nDataset Statistics:\")\nprint(f\"Number of samples: {len(dataset)}\")\nprint(f\"Average article length (words): {np.mean([len(article.split()) for article in articles]):.2f}\")\nprint(f\"Average summary length (words): {np.mean([len(highlight.split()) for highlight in highlights]):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:43:30.310978Z","iopub.execute_input":"2025-07-08T04:43:30.311366Z","iopub.status.idle":"2025-07-08T04:43:32.927229Z","shell.execute_reply.started":"2025-07-08T04:43:30.311334Z","shell.execute_reply":"2025-07-08T04:43:32.926426Z"}},"outputs":[{"name":"stdout","text":"Dataset Info:\nDataset({\n    features: ['article', 'highlights', 'id'],\n    num_rows: 2000\n})\n\nDataset Statistics:\nNumber of samples: 2000\nAverage article length (words): 601.81\nAverage summary length (words): 43.15\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Preprocessing with Deduplication and Normalization","metadata":{}},{"cell_type":"code","source":"\ndef deduplicate_dataset(dataset):\n    unique_articles = Counter(dataset['article'])\n    unique_indices = [i for i, article in enumerate(dataset['article']) if unique_articles[article] == 1 or list(unique_articles.keys()).index(article) == i]\n    return dataset.select(unique_indices)\n\ndeduped_dataset = deduplicate_dataset(dataset)\nprint(f\"Dataset size after deduplication: {len(deduped_dataset)}\")\n\n\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\ndef preprocess_function(examples):\n    # Normalize: lowercase and strip whitespace\n    inputs = [\"summarize: \" + doc.lower().strip() for doc in examples['article']]\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples['highlights'], max_length=128, truncation=True, padding=\"max_length\")\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing\nencoded_dataset = deduped_dataset.map(preprocess_function, batched=True)\n\n# Split into train and test sets\ntrain_dataset = encoded_dataset.shuffle(seed=42).select(range(int(0.8 * len(encoded_dataset))))\ntest_dataset = encoded_dataset.shuffle(seed=42).select(range(int(0.8 * len(encoded_dataset)), len(encoded_dataset)))\n\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:47:56.612223Z","iopub.execute_input":"2025-07-08T04:47:56.612577Z","iopub.status.idle":"2025-07-08T04:47:57.009779Z","shell.execute_reply.started":"2025-07-08T04:47:56.612549Z","shell.execute_reply":"2025-07-08T04:47:57.008827Z"}},"outputs":[{"name":"stdout","text":"Dataset size after deduplication: 1784\nTraining set size: 1427\nTest set size: 357\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Model Setup","metadata":{}},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:48:06.635283Z","iopub.execute_input":"2025-07-08T04:48:06.635682Z","iopub.status.idle":"2025-07-08T04:48:08.929153Z","shell.execute_reply.started":"2025-07-08T04:48:06.635649Z","shell.execute_reply":"2025-07-08T04:48:08.928092Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9937a2d6b0bb40cc8262996ae54d9ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a599f77655a4d13969c9fe26b4a03ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ad4c26222948f7b55bc32be516b42d"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=3,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\"\n)\n\n# Initialize Trainer\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=None,  # Default collator handles 'labels'\n    tokenizer=tokenizer,\n    # Removed: label_names=[\"labels\"]\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T04:48:54.537670Z","iopub.execute_input":"2025-07-08T04:48:54.538133Z","iopub.status.idle":"2025-07-08T04:51:30.310447Z","shell.execute_reply.started":"2025-07-08T04:48:54.538081Z","shell.execute_reply":"2025-07-08T04:51:30.309497Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  # else its likely a filename if supported\n<ipython-input-14-1d5735b059ab>:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [537/537 02:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.224200</td>\n      <td>1.070890</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.124600</td>\n      <td>1.060020</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.043000</td>\n      <td>1.061350</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=537, training_loss=1.2083437065394453, metrics={'train_runtime': 155.0088, 'train_samples_per_second': 27.618, 'train_steps_per_second': 3.464, 'total_flos': 579398252101632.0, 'train_loss': 1.2083437065394453, 'epoch': 3.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"## Evaluation After Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# Generate summaries for evaluation\ndevice = torch.device(\"cpu\")  # Use CPU for MacBook\nmodel.to(device)\n\ndef generate_summary_batch(batch):\n    with torch.no_grad():\n        input_ids = tokenizer(batch[\"article\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n        output = model.generate(\n            input_ids[\"input_ids\"],\n            max_length=150,\n            num_beams=5,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.95,\n            do_sample=True,\n            early_stopping=True\n        )\n        summaries = tokenizer.batch_decode(output, skip_special_tokens=True)\n    return {\"summary\": summaries}\n\nsummaries = test_dataset.map(generate_summary_batch, batched=True, batch_size=8)\n\n# Calculate ROUGE scores\ndef calculate_rouge(reference_list, generated_list):\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n    rouge_scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n    for ref, gen in zip(reference_list, generated_list):\n        scores = scorer.score(ref, gen)\n        for key in rouge_scores:\n            rouge_scores[key] += scores[key].fmeasure\n    for key in rouge_scores:\n        rouge_scores[key] /= len(reference_list)\n    return rouge_scores\n\nreference_summaries = [example[\"highlights\"] for example in test_dataset]\ngenerated_summaries = summaries[\"summary\"]  # Corrected line\n\nrouge_scores = calculate_rouge(reference_summaries, generated_summaries)\nprint(\"Post-Fine-Tuning ROUGE Scores:\")\nprint(f\"Average ROUGE-1: {rouge_scores['rouge1']:.4f}\")\nprint(f\"Average ROUGE-2: {rouge_scores['rouge2']:.4f}\")\nprint(f\"Average ROUGE-L: {rouge_scores['rougeL']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T06:10:34.171106Z","iopub.execute_input":"2025-07-08T06:10:34.171438Z","iopub.status.idle":"2025-07-08T06:10:36.037036Z","shell.execute_reply.started":"2025-07-08T06:10:34.171412Z","shell.execute_reply":"2025-07-08T06:10:36.036326Z"}},"outputs":[{"name":"stdout","text":"Post-Fine-Tuning ROUGE Scores:\nAverage ROUGE-1: 0.3191\nAverage ROUGE-2: 0.1172\nAverage ROUGE-L: 0.2229\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Define generate_summary_batch to ensure CPU usage\ndef generate_summary_batch(batch):\n    with torch.no_grad():\n        input_ids = tokenizer(batch[\"article\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n        output = model.generate(\n            input_ids[\"input_ids\"],\n            max_length=150,\n            num_beams=5,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.95,\n            do_sample=True,\n            early_stopping=True\n        )\n        summaries = tokenizer.batch_decode(output, skip_special_tokens=True)\n    return {\"summary\": summaries}\n\n# Hyperparameter tuning (grid search)\nlearning_rates = [1e-4, 3e-4]\nbatch_sizes = [2, 4]\nbest_metrics = None\nbest_lr = None\nbest_bs = None\nbest_model_path = None\n\ndevice = torch.device(\"cuda\")  # Ensure CPU usage\ntorch.cuda.empty_cache()  # Clear any CUDA cache\n\nfor lr in learning_rates:\n    for bs in batch_sizes:\n        print(f\"\\nTuning with learning_rate={lr}, batch_size={bs}\")\n        training_args = TrainingArguments(\n            output_dir=f\"./results_lr_{lr}_bs_{bs}\",\n            eval_strategy=\"epoch\",\n            learning_rate=lr,\n            per_device_train_batch_size=bs,\n            per_device_eval_batch_size=bs,\n            num_train_epochs=1,\n            weight_decay=0.01,\n            save_total_limit=1,\n            logging_dir=f\"./logs_lr_{lr}_bs_{bs}\",\n            logging_steps=10,\n            report_to=\"none\"\n        )\n        model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n        model.to(device)  # Double-check model is on CPU\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=test_dataset,\n            data_collator=None,\n            processing_class=tokenizer\n        )\n        trainer.train()\n        summaries = test_dataset.map(generate_summary_batch, batched=True, batch_size=8)\n        generated_summaries = summaries[\"summary\"]\n        rouge_scores = calculate_rouge(reference_summaries, generated_summaries)\n        metrics = {\"eval_rouge1\": rouge_scores['rouge1'], \"eval_rouge2\": rouge_scores['rouge2'], \"eval_rougeL\": rouge_scores['rougeL']}\n        print(f\"Metrics for lr={lr}, bs={bs}: {metrics}\")\n        if best_metrics is None or metrics['eval_rouge1'] > best_metrics['eval_rouge1']:\n            best_metrics = metrics\n            best_lr = lr\n            best_bs = bs\n            best_model_path = f\"./results_lr_{lr}_bs_{bs}/best_model\"\n            trainer.save_model(best_model_path)\n\nprint(f\"\\nBest Hyperparameters: learning_rate={best_lr}, batch_size={best_bs}\")\nprint(f\"Best Metrics: {best_metrics}\")\n\n# Load best model\nmodel = T5ForConditionalGeneration.from_pretrained(best_model_path).to(device)\ntokenizer = T5Tokenizer.from_pretrained(best_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:12:45.936141Z","iopub.execute_input":"2025-07-08T07:12:45.936467Z","iopub.status.idle":"2025-07-08T07:25:42.866112Z","shell.execute_reply.started":"2025-07-08T07:12:45.936437Z","shell.execute_reply":"2025-07-08T07:25:42.865408Z"}},"outputs":[{"name":"stdout","text":"\nTuning with learning_rate=0.0001, batch_size=2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [357/357 01:08, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.272000</td>\n      <td>1.090717</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ddc568ed7a45078adc2b8234b8f3e8"}},"metadata":{}},{"name":"stdout","text":"Metrics for lr=0.0001, bs=2: {'eval_rouge1': 0.30371035675317, 'eval_rouge2': 0.10734313969488142, 'eval_rougeL': 0.21192910030368214}\n\nTuning with learning_rate=0.0001, batch_size=4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='179' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [179/179 00:52, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.301900</td>\n      <td>1.127873</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b83847b4875b44b3987b518534ae7b18"}},"metadata":{}},{"name":"stdout","text":"Metrics for lr=0.0001, bs=4: {'eval_rouge1': 0.2971990753138704, 'eval_rouge2': 0.10535934405959128, 'eval_rougeL': 0.20685579900832413}\n\nTuning with learning_rate=0.0003, batch_size=2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='357' max='357' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [357/357 01:07, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.237200</td>\n      <td>1.071153</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad6c1a814474ee4a9870538a5cc47eb"}},"metadata":{}},{"name":"stdout","text":"Metrics for lr=0.0003, bs=2: {'eval_rouge1': 0.3194840316372372, 'eval_rouge2': 0.11713891447382245, 'eval_rougeL': 0.2237683616910083}\n\nTuning with learning_rate=0.0003, batch_size=4\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='179' max='179' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [179/179 00:51, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.230700</td>\n      <td>1.078225</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/357 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96414406c68641c5a6e61795c6cd73d1"}},"metadata":{}},{"name":"stdout","text":"Metrics for lr=0.0003, bs=4: {'eval_rouge1': 0.3158887426640696, 'eval_rouge2': 0.11568756738285062, 'eval_rougeL': 0.2194648552131153}\n\nBest Hyperparameters: learning_rate=0.0003, batch_size=2\nBest Metrics: {'eval_rouge1': 0.3194840316372372, 'eval_rouge2': 0.11713891447382245, 'eval_rougeL': 0.2237683616910083}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# Inference function\ndef summarize_text(text, model, tokenizer, max_length=150):\n    inputs = tokenizer(\"summarize: \" + text.lower().strip(), return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n    with torch.no_grad():\n        summary_ids = model.generate(\n            inputs[\"input_ids\"],\n            max_length=max_length,\n            num_beams=5,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.95,\n            do_sample=True,\n            early_stopping=True\n        )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# Display examples\nfor i in range(3):\n    print(f\"\\nExample {i+1}:\")\n    print(\"Article:\", test_dataset[i][\"article\"][:500] + \"...\")\n    print(\"Original Summary:\", test_dataset[i][\"highlights\"])\n    print(\"Generated Summary:\", summarize_text(test_dataset[i][\"article\"], model, tokenizer))\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T07:25:42.867183Z","iopub.execute_input":"2025-07-08T07:25:42.867432Z","iopub.status.idle":"2025-07-08T07:25:45.470744Z","shell.execute_reply.started":"2025-07-08T07:25:42.867410Z","shell.execute_reply":"2025-07-08T07:25:45.469767Z"}},"outputs":[{"name":"stdout","text":"\nExample 1:\nArticle: (CNN)  -- Canadian Prime Minister Stephen Harper said Thursday that Canada's governor general has allowed him to suspend Parliament, postponing a no-confidence vote from his opponents that he was likely to lose. Canadian Prime Minister Stephen Harper says Parliament will resume on January 26. Harper called on his opponents to work with his government on measures to aid the nation's economy when Parliament returns on January 26. \"The first order of business will be the presentation of a federal b...\nOriginal Summary: NEW: Opposition accuses PM Harper of putting his job ahead of Canada's interests .\nMove postpones opposition parties' plan for no-confidence vote next week .\nLiberal and New Democratic parties join with Bloc Quebecois to try to unseat Tories .\nVote likely would have brought down Canada's Conservative government .\nGenerated Summary: Canadian prime minister stephen Harper says he will suspend parliament. Harper calls on opponents to work on measures to aid the nation's economy. Governor General michaelle Jean denies Harper's request.\n\n\n\nExample 2:\nArticle: (CNN) -- At least 14 people were killed and 60 others wounded Thursday when a bomb ripped through a crowd waiting to see Algeria's president in Batna, east of the capital of Algiers, the Algerie Presse Service reported. A wounded person gets first aid shortly after Thursday's attack in Batna, Algeria. The explosion occurred at 5 p.m. about 20 meters (65 feet) from a mosque in Batna, a town about 450 kilometers (280 miles) east of Algiers, security officials in Batna told the state-run news agenc...\nOriginal Summary: Bomb victims waiting for presidential visit .\nBlast went off 15 minutes before president's arrival .\nAlgeria faces Islamic insurgency .\nAl Qaeda-affiliated group claimed July attacks .\nGenerated Summary: Bomb ripped through crowd waiting to see algeria's president in Batna. Bomb went off 15 minutes before expected arrival of President Abdel-Aziz bouteflika. No immediate claim of responsibility for the bombing.\n\n\n\nExample 3:\nArticle: (CNN) -- Since 2½-year-old Ava Zinna ended up in the emergency room  this summer after an allergic reaction to peanuts, her mother, Tara, has worried about her daughter's food whenever they eat out. But when the family went to Blue Smoke restaurant Sunday afternoon in New York, someone had already asked to hold the peanuts. Ava Zinna ate an allergen-free meal at the Worry Free Dinners event on Sunday. The Zinnas took part in Worry Free Dinners, a series of monthly meals for people with food alle...\nOriginal Summary: Woman organizes dinners at restaurants for people with food allergies .\nIf you have a food allergy, call ahead and tell the restaurant management, she says .\n12 million Americans have food allergies, though some kids outgrow them .\nFood allergies are on the rise, but no one is sure why .\nGenerated Summary: ava Zinna ate an allergen-free meal at the worry free dinners event. The event was aimed directly at parents and children affected by food allergies. Food manufacturers have been required to clearly label products that contain any of the most common allergens.\n\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}